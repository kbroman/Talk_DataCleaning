\documentclass[aspectratio=169,12pt,t]{beamer}
\usepackage{graphicx}
\setbeameroption{hide notes}
\setbeamertemplate{note page}[plain]
\usepackage{listings}
\usepackage{eepic}

\input{header.tex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% end of header
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% title info
\title{data cleaning principles}
\author{\vspace*{-10pt} \href{https://kbroman.org}{Karl Broman}}
\institute{Biostatistics \& Medical Informatics, UW{\textendash}Madison}
\date{\href{https://twitter.com/kwbroman}{\tt \scriptsize \color{foreground} @kwbroman}
\\[-4pt]
\href{https://kbroman.org}{\tt \scriptsize \color{foreground} kbroman.org}
\\[-4pt]
\href{https://github.com/kbroman}{\tt \scriptsize \color{foreground} github.com/kbroman}
\\[-4pt]
{\scriptsize \href{https://kbroman.org/Talk_DataCleaning}{\tt kbroman.org/Talk\_DataCleaning}}
}


\begin{document}

% title slide
{
\setbeamertemplate{footline}{} % no page number here
\frame{
  \titlepage

  \vfill \hfill
  \href{https://creativecommons.org/publicdomain/zero/1.0/}{\includegraphics[height=7mm]{Figs/cc-zero.png}}

  \note{
    These are slides for a talk for the csv,conf,v6 ({\tt
    https://csvconf.com/}) on May 4--5, 2021.

    Data analysts spend a lot of time organizing and cleaning data,
    but few of us have been trained to do so. Why is that?

    Some say that data cleaning is difficult to generalize. But I
    think there are some general principles. Moreover, I think we have
    an important shared experience in data cleaning that we can
    commiserate about, and through which we can learn from each other.
  }

} }


\begin{frame}[c]{}

\centering
\Large

Tidy data are all alike, \\[3pt]
but every messy dataset  \\[3pt]
is messy in its own way.

\bigskip\bigskip

\hspace{5cm} --
\href{https://r4ds.had.co.nz/tidy-data.html}{Hadley Wickham}



\note{
  Hadley's talking more about data organization than data
  cleanliness.  And his point is that if you make data tidy, it
  simplifies all the downstream analyses.

  But {\hilit is} every messy dataset {\vhilit uniquely} messy?

  For sure, many my collaborators have shown impressive creativity in
  their approach to organizing and managing data. But we do see
  many of the same sorts of problems over and over.
}
\end{frame}


\begin{frame}[c]{}

\centering
\Large

If I clean up [Medicare] data ...\\[3pt]
does any of the knowledge I gain ...\\[3pt]
apply to the processing of RNA-seq data?


\bigskip\bigskip

\hspace{5cm} --
\href{https://doi.org/10.1080/10618600.2017.1385470}{Roger Peng}



\note{
  In his discussion of David Donoho's paper about data science, Roger
  Peng wrote about how data cleaning is frustratingly difficult to
  generalize.

  But my answer to his question is {\vhilit absolutely!}

  A person with experience cleaning one dataset has important
  experience to draw upon when moving to another dataset, even if it's
  of a totally different nature.
}
\end{frame}



\begin{frame}[c]{}


  \href{https://datamishapsnight.com}{\figh{Figs/data_mishaps_night.png}{1.0}}

  \vspace{-7mm}
  \hfill
  \begin{minipage}[c]{0.33\textwidth}
    {\footnotesize Caitlin Hudon \& Laura Ellis}

    \href{https://datamishapsnight.com}{\lolit \tt dataMishapsNight.com}
  \end{minipage}

\note{
  In February, 2021, Caitlin Hudon and Laura Ellis organized a
  Friday evening conference where 16 people gave short presentations on data
  mishaps.

  Many of the stories concerned mistakes in data cleaning, and these
  seemed to bring out a strong sense of shared experience. We have
  suffered and struggled through very similar data problems.
}
\end{frame}



\begin{frame}{Data cleaning}


\vspace*{-6pt}

\begin{minipage}[t]{0.45\textwidth}

\bbi
\item tedious
\item embarrassing
\item needs context
\item doesnâ€™t feel like progress
  \ei

\end{minipage} \hfill
\begin{minipage}[t]{0.45\textwidth}

\only<2>{
\bbi
\item requires creativity
\item requires coding prowess
\item source of many problems
\ei
}
\end{minipage}

\note{
  Really, I think we don't usually teach data cleaning
  because it's something we prefer to keep private.

  We're shy about it.

  And data cleaning code is our ugliest code.
}
\end{frame}



\begin{frame}[c]{Data cleaning principles}

\bigskip

  \figh{Figs/logo.pdf}{0.68}

\note{
  I'm proposing a set of basic principles for data cleaning, and
  splitting them into five groups. There are some fundamental
  principles, followed by four basic ideas: verify things that you
  expect, explore to find further oddities, ask questions, and
  document what you've done.
}
\end{frame}


\begin{frame}{}
\lfigh{Figs/logo_fundamentals.pdf}{0.2}

\bigskip\bigskip\bigskip

\centering
\Large

{\princolorA 1. Don't clean data when you're tired or hungry.}

\bigskip\bigskip

\large
\hspace{5cm}
(paraphrasing \href{https://twitter.com/ghazalgulati}{Ghazal Gulati})


\note{
  At her talk at the Data Mishaps night, Ghazal Gulati emphasized this
  point, of not cleaning data when you're tired or hungry.

  Data cleaning requires considerable concentration, and you need to
  allow sufficient time to do the work. If you're in a hurry, you'll
  miss things.
}
\end{frame}


\begin{frame}{}
\lfigh{Figs/logo_fundamentals.pdf}{0.2}

\bigskip\bigskip\bigskip


\only<1|handout 0>{
\centering
\Large

{\princolorA 2. Don't trust anyone (even yourself)}
}

\only<2>{
  \vspace*{-27.5mm}
  \hspace{0.4\textwidth} {\princolorA 2. Don't trust anyone (even yourself)}

\vspace{0.25\textheight}

\centering
\large

``my motto is `trust no one' \hspace{1cm} \\[3pt]
\hspace{1cm} ...except maybe @kwbroman?''

\bigskip

\hspace{5cm}
-- \href{https://twitter.com/JennyBryan/status/595689693162938368}{Jenny Bryan}
}

\note{
   Next: don't trust anyone. Even if the initial data cleaning was
   done by someone you respect, you should double-check things that
   they may have missed. And data cleaning is an ongoing process.

   Jenny Bryan's tweet is among the nicest things anyone has said
   about me.
}
\end{frame}



\begin{frame}{}
\lfigh{Figs/logo_fundamentals.pdf}{0.2}

  \vspace*{-15mm}
  \hspace{0.4\textwidth} {\princolorA 3. Think about what might have gone wrong} \\
  \hspace{0.4\textwidth} {\color{background} 3.}
            {\princolorA and how it might be revealed} \hspace{20.5mm}

\vspace{0.06\textheight}

\figh{Figs/sample_mixups.png}{0.68}

\vfill
\hfill \href{https://doi.org/10.1534/g3.115.019778}{\scriptsize
 \tt doi:10.1534/g3.115.019778}


\note{
  Personally, I think this is the most important principle for data
  cleaning. It has been central in guiding my approach.

  The figure here is an illustration of the most startlingly result
  I've had in data cleaning: a genetics project where almost 20\% of the
  DNA samples had been mislabelled. The samples were arranged in wells
  in 8$\times$12 plates; four of the six plates are shown here. The
  dots indicate the correct DNA was placed in the well, but the arrows
  point from where a sample should have been to where it actually was
  placed.

  I ultimately came to this finding by thinking about what might have
  gone wrong in the project, checking for particularly problems, and
  then following the trail of evidence to this mess.
}
\end{frame}


\begin{frame}{}
\lfigh{Figs/logo_fundamentals.pdf}{0.2}

\vspace{-15mm} \hspace{0.4\textwidth}
{\princolorA 4. Use care in merging}


\vspace{0.12\textheight}

\includegraphics[height=0.55\textheight]{Figs/spreadsheet_colnames.pdf}

\vspace*{-0.40\textheight}
\hspace*{0.18\textwidth}
\includegraphics[height=0.55\textheight]{Figs/spreadsheet_colnames2.pdf}


\note{
  Many problems arise due to mistakes when merging data from multiple
  files. A common problem is a change in the data arrangement, such as
  in the order of columns.

  Focus on the labels (which are more likely correct), rather than the
  position of variables in a file (which are more likely to change).
}
\end{frame}


\begin{frame}{}
\lfigh{Figs/logo_fundamentals.pdf}{0.2}

\bigskip\bigskip\bigskip

\centering
\Large

{\princolorA 5. Dates \& categories suck}


\note{
  The fifth fundamental principle is that dates and categories suck.
  You'll expend an inordinate amount of time dealing with these: typos
  in category labels, different date formats, people who died a decade
  they were born or lived to be 150.

  Just be glad if you're not dealing with time zones.

  But you may be asking yourself, ``How is this a principle.''

}
\end{frame}


\begin{frame}{}

\bigskip \bigskip \bigskip \bigskip

\Large

{\color{title} Principle:} \hspace{50mm}

\centering

\bigskip \bigskip

{\hilit a fundamental truth that guides our thinking}


\note{
  Yeah, are these principles? I was thinking the same thing. Was I
  drifting away from principles and more to just stuff to know or do?

  This seems a pretty good definition, and is sufficiently broad to
  cover what I'm proposing, for the most part.
}
\end{frame}


\begin{frame}{}
\lfigh{Figs/logo_fundamentals.pdf}{0.2}

\bigskip\bigskip\bigskip

\centering
\Large

{\princolorA 5. Dates \& categories suck}


\note{
  So yeah, this counts as a principle.

  Much of your pain will come from the dates and categorical data;
  you should be ready for that.
}
\end{frame}




\begin{frame}{}
\lfigh{Figs/logo_verify.pdf}{0.2}

\vspace{-15mm} \hspace{0.4\textwidth}
{\princolorB 6. Check that distinct things are distinct}

\vspace{0.16\textheight}

\figh{Figs/dup_ids.pdf}{0.60}


\note{
   The next major section of principles concern efforts to
   {\nhilit verify} all of the things that should be true for your data.

   First up: are the variables that are supposed to have distinct
   values really showing distinct values? Here, there are a pair of
   individual identifiers that are duplicated.
}
\end{frame}


\begin{frame}{}
\lfigh{Figs/logo_verify.pdf}{0.2}

\vspace{-15mm} \hspace{0.4\textwidth}
{\princolorB 7. Check that matching things match}

\vspace{0.22\textheight}

\begin{minipage}[t]{0.48\textwidth}

\includegraphics[width=\textwidth]{Figs/mismatchB.pdf}
\end{minipage}
\hfill
\begin{minipage}[t]{0.48\textwidth}

\includegraphics[width=\textwidth]{Figs/mismatch.pdf}
\end{minipage}


\note{
    Next, are the things that are supposed to match actually matching?
    This often concerns data that are repeated in multiple data files.
    For example, you might have body weight included multiple times;
    are individuals' weights the same in all files? Here, there's a
    column for ``number of generations'' that shows an inconsistency
    between two files.
}
\end{frame}



\begin{frame}{}
\lfigh{Figs/logo_verify.pdf}{0.2}

\vspace{-15mm} \hspace{0.4\textwidth}
{\princolorB 8. Check calculations}

\vspace{0.18\textheight}

\figh{Figs/check_calculations.pdf}{0.65}

\note{
    Any time there is a calculation, you should verify the values.
    This is useful both for ensuring that you understand the
    calculation, and you also have the chance to reduce round-off
    error (though that seldom matters). Occasionally you may see
    mistakes.

    Here, I'm plotting HOMA-IR which is the product of blood glucose
    and blood insulin; the values in the data on the x-axis and the
    values I calculated on the y-axis. It's often better to look at the
    differences directly; in the right panel I plot the differences on
    the y-axis. It appears that there is a batch of individuals that
    whose values were rounded more coarsely.

    Note that rather than omitting missing values, I've pulled them
    out and plotted them in the margins. Showing the missing values
    can be really important for identifying problems. Here there are
    some HOMA-IR values that are missing but maybe shouldn't be.

    {\nvhilit Note}: when I presented this at csv,conf,v6, I mistakenly said
    that HOMA-IR is the ratio of glucose to insulin, and the labels on
    the plots were wrong. Oops!
}
\end{frame}



\begin{frame}{}
\lfigh{Figs/logo_verify.pdf}{0.2}

\centering
\Large

\bigskip \bigskip \bigskip

{\princolorB 9. Look for other instances of a problem}

\note{
    As with software testing, any time you find a problem, be sure to
    look for other instances of that problem.
}
\end{frame}



\begin{frame}{}
\lfigh{Figs/logo_explore.pdf}{0.2}

\vspace{-15mm} \hspace{0.4\textwidth}
{\princolorC 10. Make lots of plots}

\vspace{0.15\textheight}

\only<1>{\figh{Figs/il3.pdf}{0.75}}
\only<2|handout 0>{\figh{Figs/body_weight.pdf}{0.75}}
\only<3|handout 0>{\figh{Figs/adipose_weight.pdf}{0.75}}


\note{
    A particularly important aspect of data cleaning concerns just
    exploring the data to identify potential problems. And a
    particularly important aspect of that exploration is to just make
    lots of plots.

    Plot variables over time or by subject ID, which may indicate
    things like batch effects.

    Make scatterplots of variables against each other, looking
    particularly for outliers. Outliers could be real biological
    variation, but they could also be data entry problems, like a pair
    of numbers being swapped, or the weights entered in grams rather
    than milligrams.
}
\end{frame}


\begin{frame}{}
\lfigh{Figs/logo_explore.pdf}{0.2}

\vspace{-15mm} \hspace{0.4\textwidth}
{\princolorC 11. Look at missing value patterns}

\vspace{0.12\textheight}

\figh{Figs/scatter_na.pdf}{0.72}

\note{
    The pattern of missing data can be particularly informative about
    problems. Two particularly useful tools are the R packages
    visdat and naniar.


    In the left panel, visdat (https://docs.ropensci.org/visdat/)
    provides a heatmap indicating which data points are missing, and
    also the variable types.

    In the right panel, naniar (http://naniar.njtierney.com/)
    provides a scatterplot that include the cases that are missing one
    or both variables.
}
\end{frame}


\begin{frame}{}
\lfigh{Figs/logo_explore.pdf}{0.2}

\vspace{-15mm} \hspace{0.4\textwidth}
       {\princolorC 12. With massive data,} \\
\hspace{0.4\textwidth} {\color{background} 12.} {\princolorC make more plots not fewer}

\vspace{0.10\textheight}

\only<1|handout 0>{\figh{Figs/hypo_arrays.pdf}{0.70}}
\only<2|handout 0>{\figh{Figs/hypo_arrays_hilit.pdf}{0.70}}
\only<3|handout 0>{\figh{Figs/hypo_iqr_v_median.pdf}{0.70}}
\only<4>{\figh{Figs/hypo_boxplots.pdf}{0.70}}


\note{
  With large-scale datasets, it can be hard to make the sort of
  exploratory plots that you'd typically make. With oodles of data,
  you'd think you'd be looking at oodles of plots, but there's a
  tendency to give up and not look at any.

  It's hard to look at 500 histograms, but it can be done. Superimpose
  a bunch of density estimates, maybe highlighting some portion of
  them. You can also pull out a couple of summary statistics, such as
  the median and inter-quartile range.

  Or here I'm looking at the equivalent of 500 boxplots. I sorted a
  set of gene expression microarrays by their median, and then plotted
  the median in blue, the 25th and 75th percentile in black, the 10th
  and 90th in green, 5th and 95th in red, and 1st and 99th in purple.

  With these data, it became apparent that there were 120 badly
  behaved arrays, with median shifted to the right and with a long
  left tail.
}
\end{frame}


\begin{frame}{}
\lfigh{Figs/logo_explore.pdf}{0.2}

\vspace{-15mm} \hspace{0.4\textwidth}
{\princolorC 13. Follow up all artifacts}


\vspace{0.10\textheight}

\figh{Figs/weird_correlation_matrix.png}{0.75}

\hfill
{
\scriptsize
\href{https://kbroman.org/blog/2012/04/25/microarrays-suck/}{\tt kbroman.org/blog/2012/04/25/microarrays-suck}
}

\note{
  Wow the clash of those colors is particularly bad.

  This is a heat map of the correlation matrix for a set of gene
  expression microarrays. The plaid pattern was a shock to me, and
  was caused by a set of bad arrays that we hadn't noticed previously.

  My point here is simply to follow up all artifacts.

  If you see something weird, follow through and try to figure out the
  underlying cause. If could be an error, or a set of bad assays, or
  it could be the most interesting finding in the study.
}
\end{frame}


\begin{frame}{}
\lfigh{Figs/logo_ask.pdf}{0.2}

\hfill
\begin{minipage}[t]{0.9\textwidth}


\bbi
\princolorD
\item[\princolorD 14.] Ask questions
\item[\princolorD 15.] Ask for the primary data
\item[\princolorD 16.] Ask for metadata
\item[\princolorD 17.] Ask why data are missing
\ei

\end{minipage}

\note{
    I'm sure I'll be running out of time at this point, and so I've
    condensed these last two sections.

    First, don't be shy about asking questions: about the data, to get the
    primary data, to also get metadata that explains the data
    (particularly variable names and such).

    Among the most important questions to ask are why data are
    missing (for example, that an assay didn't work, that the value
    was too low, or that the value was too high).
}
\end{frame}


\begin{frame}{}
\lfigh{Figs/logo_document.pdf}{0.2}

\hfill
\begin{minipage}[t]{0.9\textwidth}


\bbi
\princolorE
\item[\princolorE 18.] Create checklists \& pipelines
\item[\princolorE 19.] Document not just what but why
\item[\princolorE 20.] Expect to recheck
\ei

\end{minipage}

\note{
  The final section of principles concerns documentation.

  Create checklists and pipelines for yourself and others, so that
  when you return to similar data, you will remember many of the
  things to check, and you can build on what you've learned.

  And for this sort of work, we need things to be {\nhilit more} than
  reproducible; you will need to capture not just what you did, but
  also {\nhilit why}. For example, if you decide to omit some subset
  of samples, will you remember 2 years from now just {\nhilit why}
  you chose to omit them?

  Finally, data cleaning is not a single step in the analysis chain;
  rather, it is an ongoing process that you will need to continually
  revisit as you delve deeper into the data. Keep an eye out for
  hints of problems, and arrange your work with the expectation that
  you'll need to re-run everything at some point.
}
\end{frame}



{
\setbeamertemplate{footline}{} % no page number here

\begin{frame}[c]{Data cleaning principles}

\bigskip

  \figh{Figs/logo_withtext.pdf}{0.88}

\note{
  In summary, data cleaning is not an activity that needs to be
  constructed from scratch in each instance for each dataset. There
  are a number of principles that can guide our approach to cleaning
  data.

  There may actually be more commonalities in our data cleaning
  experiences and methods than in the following stages of our data
  analysis work.
}
\end{frame}
}


\begin{frame}[c]{}

\centering
\Large

I will let the data speak for itself \\[3pt]
when it cleans itself.

\bigskip\bigskip

\hspace{5cm} --
\href{https://twitter.com/AllisonReichel}{Allison Reichel}



\note{
  I love this.

  The proportion of our time spent cleaning data is likely to
  increase. It's never going to clean itself.
}
\end{frame}



\begin{frame}{}

\Large

Slides: \href{https://kbroman.org/Talk_DataCleaning}{\tt kbroman.org/Talk\_DataCleaning}

\vspace*{-7mm}
\hfill
\href{https://creativecommons.org/publicdomain/zero/1.0/}{\includegraphics[height=7mm]{Figs/cc-zero.png}}

\vspace{-1mm}

\href{https://kbroman.org}{\tt \lolit kbroman.org}

\vspace{1mm}

\href{https://github.com/kbroman}{\tt \lolit github.com/kbroman}

\vspace{1mm}

\href{https://twitter.com/kwbroman}{\tt \lolit @kwbroman}



\vspace*{-4mm} \hspace*{50mm}
\includegraphics[height=0.65\textheight]{Figs/logo_withtext.pdf}


\note{
  Here's where you can find me and these slides.
}

\end{frame}




\end{document}
